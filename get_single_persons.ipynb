{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "outputs": [],
   "source": [
    "!pip install wandb\n",
    "!pip install ultralytics\n",
    "import wandb\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3etHeg2aVdpo",
   "metadata": {
    "id": "3etHeg2aVdpo"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import pathlib\n",
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andrewmvd/face-mask-detection\")\n",
    "os.rename(path, \"face_mask_dataset\")\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    return root\n",
    "def get_yolo_bbox(xml_bbox, width, height) -> str:\n",
    "    x_center = (xml_bbox[0] + xml_bbox[2]) / 2 / width\n",
    "    y_center = (xml_bbox[1] + xml_bbox[3]) / 2 / height\n",
    "    w = (xml_bbox[2] - xml_bbox[0]) / width\n",
    "    h = (xml_bbox[3] - xml_bbox[1]) / height\n",
    "    return \" \".join([str(i) for i in [x_center, y_center, w, h]])\n",
    "def convert_to_txt() -> None:\n",
    "    data = \"face_mask_dataset/annotations\"\n",
    "    output = \"datasets/labels\"\n",
    "    classes = [\"with_mask\", \"mask_weared_incorrect\", \"without_mask\"]\n",
    "\n",
    "    for file in os.listdir(data):\n",
    "        root = ET.parse(f\"{data}/{file}\").getroot()\n",
    "        width = int(root.find(\"size\").find(\"width\").text)\n",
    "        height = int(root.find(\"size\").find(\"height\").text)\n",
    "        boxes = []\n",
    "\n",
    "        for i in root.findall(\"object\"):\n",
    "            class_id = classes.index(i.find(\"name\").text)\n",
    "            xml_bbox = [int(coords.text) for coords in i.find(\"bndbox\")]\n",
    "            yolo_bbox = get_yolo_bbox(xml_bbox, width, height)\n",
    "            boxes.append((class_id, yolo_bbox))\n",
    "        with open(f\"{output}/{file.split('.')[0]}.txt\", \"w\") as f:\n",
    "            for class_id, yolo_bbox in boxes:\n",
    "                f.write(f\"{class_id} {yolo_bbox}\\n\")\n",
    "\n",
    "if not os.path.exists(\"datasets\"):\n",
    "    os.mkdir(\"datasets\")\n",
    "    os.mkdir(\"datasets/labels\")\n",
    "    os.mkdir(\"datasets/train\")\n",
    "    os.mkdir(\"datasets/val\")\n",
    "    os.mkdir(\"datasets/test\")\n",
    "    os.mkdir(\"datasets/train/images\")\n",
    "    os.mkdir(\"datasets/val/images\")\n",
    "    os.mkdir(\"datasets/test/images\")\n",
    "    os.mkdir(\"datasets/train/labels\")\n",
    "    os.mkdir(\"datasets/val/labels\")\n",
    "    os.mkdir(\"datasets/test/labels\")\n",
    "\n",
    "def create_training_set(images) -> None:\n",
    "    for i, image in enumerate(images):\n",
    "        if i < 0.7 * len(images):\n",
    "            os.rename(f\"face_mask_dataset/images/{image}\", f\"datasets/train/images/{image}\")\n",
    "            os.rename(f\"datasets/labels/{image.split('.')[0]}.txt\", f\"datasets/train/labels/{image.split('.')[0]}.txt\")\n",
    "        elif i < 0.85 * len(images):\n",
    "            os.rename(f\"face_mask_dataset/images/{image}\", f\"datasets/val/images/{image}\")\n",
    "            os.rename(f\"datasets/labels/{image.split('.')[0]}.txt\", f\"datasets/val/labels/{image.split('.')[0]}.txt\")\n",
    "        else:\n",
    "            os.rename(f\"face_mask_dataset/images/{image}\", f\"datasets/test/images/{image}\")\n",
    "            os.rename(f\"datasets/labels/{image.split('.')[0]}.txt\", f\"datasets/test/labels/{image.split('.')[0]}.txt\")\n",
    "\n",
    "yolo_yaml = f\"\"\"train: {pathlib.Path(\"datasets\").resolve().as_posix()}/train/images\n",
    "val: {pathlib.Path(\"datasets\").resolve().as_posix()}/val/images\n",
    "nc: 3\n",
    "names: ['with_mask', 'mask_weared_incorrect', 'without_mask']\n",
    "\"\"\"\n",
    "\n",
    "with open(\"yolo.yaml\", \"w\") as f:\n",
    "    f.write(yolo_yaml)\n",
    "convert_to_txt()\n",
    "images = os.listdir(\"face_mask_dataset/images\")\n",
    "create_training_set(images)\n",
    "os.rmdir(\"datasets/labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598792be656b243",
   "metadata": {
    "id": "6598792be656b243"
   },
   "outputs": [],
   "source": [
    "# wandb.init(project='SeoulTechML', name='yolo11m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a8d03e4a831c9c",
   "metadata": {
    "id": "c2a8d03e4a831c9c"
   },
   "outputs": [],
   "source": [
    "ultralytics.SETTINGS['loggers'] = ['wandb']\n",
    "ultralytics.SETTINGS['wandb'] = {'project': 'SeoulTechML', 'name': 'yolo11m'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940281d58399e87",
   "metadata": {
    "id": "d940281d58399e87"
   },
   "outputs": [],
   "source": [
    "# model = ultralytics.YOLO('yolo11m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265af394123af6f",
   "metadata": {
    "id": "3265af394123af6f"
   },
   "outputs": [],
   "source": [
    "# results = model.train(data=\"yolo.yaml\", epochs=10, save=True, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b293681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "# Define the Data dataclass\n",
    "@dataclass\n",
    "class Data:\n",
    "    single_person_files: list[str]\n",
    "    with_mask: list[str]\n",
    "    without_mask: list[str]\n",
    "\n",
    "# Define the JSON file path\n",
    "json_path = 'single_files.json'\n",
    "\n",
    "# Create the file with default data\n",
    "default_data = {\n",
    "    \"single_person_files\": [],\n",
    "    \"with_mask\": [],\n",
    "    \"without_mask\": []\n",
    "}\n",
    "with open(json_path, 'w') as file:\n",
    "    json.dump(default_data, file, indent=2)\n",
    "print(f\"File '{json_path}' has been created with default data.\")\n",
    "\n",
    "# Load the JSON data\n",
    "with open(json_path, 'r') as file:\n",
    "    raw_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7b072261855ad",
   "metadata": {
    "id": "42f7b072261855ad"
   },
   "outputs": [],
   "source": [
    "final_model = ultralytics.YOLO('best_50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351670ee8c10986b",
   "metadata": {
    "collapsed": true,
    "id": "351670ee8c10986b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data to start with the existing structure\n",
    "with open(json_path, 'r') as file:\n",
    "    aggregated_data = json.load(file)\n",
    "\n",
    "# Function to split a list into chunks\n",
    "def split_list(a, n):\n",
    "    for i in range(0, len(a), n):\n",
    "        yield a[i:i+n]\n",
    "\n",
    "# Define the number of iterations\n",
    "num_iterations = 5\n",
    "\n",
    "# Perform the loop x times\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Starting Iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    # Temporary data for the current iteration\n",
    "    iteration_data = {\n",
    "        \"single_person_files\": [],\n",
    "        \"with_mask\": [],\n",
    "        \"without_mask\": []\n",
    "    }\n",
    "\n",
    "    # Loop through the sets\n",
    "    for set_type in [\"train\", \"val\", \"test\"]:\n",
    "        images = [f\"datasets/{set_type}/images/{i}\" for i in os.listdir(f\"datasets/{set_type}/images\")]\n",
    "        for chunk in split_list(images, 50):  # Process images in batches of 50\n",
    "            predict = final_model.predict(chunk)  # Predict using the model\n",
    "            for p in predict:\n",
    "                filename = p.path.split(\"/\")[-1]\n",
    "                if len(p.boxes.cls) == 1:  # Ensure there is exactly one detected class\n",
    "                    iteration_data[\"single_person_files\"].append(filename)\n",
    "                    if p.boxes.cls[0] == 0:  \n",
    "                        iteration_data[\"with_mask\"].append(filename)\n",
    "                    elif p.boxes.cls[0] == 2:  \n",
    "                        iteration_data[\"without_mask\"].append(filename)\n",
    "\n",
    "    print(f\"Finished Iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    # Aggregate results into the JSON structure\n",
    "    aggregated_data[\"single_person_files\"].extend(iteration_data[\"single_person_files\"])\n",
    "    aggregated_data[\"with_mask\"].extend(iteration_data[\"with_mask\"])\n",
    "    aggregated_data[\"without_mask\"].extend(iteration_data[\"without_mask\"])\n",
    "\n",
    "# Remove duplicates\n",
    "final_data = {\n",
    "    key: list(np.unique(values))  # Use np.unique to remove duplicates\n",
    "    for key, values in aggregated_data.items()\n",
    "}\n",
    "\n",
    "# Save the aggregated data back to the JSON file\n",
    "with open(json_path, \"w\") as file:\n",
    "    json.dump(final_data, file, indent=2)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Final Single person files: {len(final_data['single_person_files'])}\")\n",
    "print(f\"Final With mask: {len(final_data['with_mask'])}\")\n",
    "print(f\"Final Without mask: {len(final_data['without_mask'])}\")\n",
    "print(f\"Aggregated data saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9edcf3fc07d515",
   "metadata": {
    "id": "aa9edcf3fc07d515"
   },
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "#with open(\"single_files.json\", \"w\") as f:\n",
    " # json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "-i2eu2Re0Ded",
   "metadata": {
    "id": "-i2eu2Re0Ded"
   },
   "outputs": [],
   "source": [
    "import torch as pt\n",
    "import torchvision\n",
    "\n",
    "#code here yipeee"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
